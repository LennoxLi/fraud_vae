{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data import make_dataloader, make_dataset\n",
    "from train_mlp import train, evaluate_model, calculate_metric\n",
    "from models import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/creditcard.csv\"\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = make_dataset(data_path, \n",
    "                                                        normalization=True, \n",
    "                                                        test_split_ratio=0.2)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader, val_dataloader, test_dataloader = make_dataloader(batch_size, train_dataset, True), \\\n",
    "                                                        make_dataloader(batch_size, val_dataset, False), \\\n",
    "                                                            make_dataloader(batch_size, test_dataset, False),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline w/o sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, y_train = train_dataset.tensors[0].numpy(), train_dataset.tensors[1].numpy()\n",
    "X_val, y_val = val_dataset.tensors[0].numpy(), val_dataset.tensors[1].numpy()\n",
    "X_test, y_test = test_dataset.tensors[0].numpy(), test_dataset.tensors[1].numpy()\n",
    "\n",
    "log_reg = LogisticRegression(penalty='l2', C=1.0, max_iter=100000, random_state=53,)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_val_pred = log_reg.predict(X_val)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "print(\"Trainset\\n\", calculate_metric(None, None, 'cls_report', y_train, y_train_pred))\n",
    "print(\"Valset\\n\", calculate_metric(None, None, 'cls_report', y_val, y_val_pred))\n",
    "print(\"Testset\\n\", calculate_metric(None, None, 'cls_report', y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=3000,       # Number of boosting rounds\n",
    "    max_depth=6,            # Maximum tree depth for base learners\n",
    "    learning_rate=0.05,      # Boosting learning rate\n",
    "    objective='binary:logistic',  # Binary classification\n",
    "    eval_metric='aucpr',    # Evaluation metric AUC-PR\n",
    "    use_label_encoder=False, # To avoid warnings in newer versions of xgboost\n",
    "    n_jobs = 12,\n",
    "    device = \"cuda\",\n",
    "    verbosity = 1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_val_pred = xgb_model.predict(X_val)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"Trainset\\n\", calculate_metric(None, None, 'cls_report', y_train, y_train_pred))\n",
    "print(\"Valset\\n\", calculate_metric(None, None, 'cls_report', y_val, y_val_pred))\n",
    "print(\"Testset\\n\", calculate_metric(None, None, 'cls_report', y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_dataset.tensors[0].shape[1]\n",
    "hidden_size = 256\n",
    "num_layers = 4\n",
    "dropout_rate = .3\n",
    "\n",
    "lr = 3e-4\n",
    "num_epochs = 20\n",
    "\n",
    "exp_name = \"baseline-mlp\"\n",
    "\n",
    "model = MLP(input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout_rate=dropout_rate,)\n",
    "\n",
    "train_loss_hist, train_acc_hist, val_acc_hist, model = train(model, train_dataloader, val_dataloader, \n",
    "      num_epochs=num_epochs, optimizer=torch.optim.Adam(model.parameters(), lr=lr), \n",
    "      criterion=torch.nn.BCELoss(), seed=53, metric=\"f1\", exp_name=exp_name )\n",
    "\n",
    "evaluate_model(exp_name, model, train_loss_hist, train_acc_hist, val_acc_hist, test_dataloader, \"cls_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w/ re-sampling (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original trainset distribution Counter({0.0: 226013, 1.0: 372})\n",
      "after applying SMOTE: Counter({0.0: 226013, 1.0: 226013})\n"
     ]
    }
   ],
   "source": [
    "from imbalance import augment_trainset\n",
    "a_smote_train_dataset = augment_trainset(train_dataset, method=\"smote\")\n",
    "a_smote_train_dataloader = make_dataloader(batch_size, a_smote_train_dataset, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, y_train = a_smote_train_dataset.tensors[0].numpy(), a_smote_train_dataset.tensors[1].numpy()\n",
    "X_val, y_val = val_dataset.tensors[0].numpy(), val_dataset.tensors[1].numpy()\n",
    "X_test, y_test = test_dataset.tensors[0].numpy(), test_dataset.tensors[1].numpy()\n",
    "\n",
    "log_reg = LogisticRegression(penalty='l2', C=1.0, max_iter=100000, random_state=53,)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_val_pred = log_reg.predict(X_val)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "print(\"Trainset\\n\", calculate_metric(None, None, 'cls_report', y_train, y_train_pred))\n",
    "print(\"Valset\\n\", calculate_metric(None, None, 'cls_report', y_val, y_val_pred))\n",
    "print(\"Testset\\n\", calculate_metric(None, None, 'cls_report', y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=3000,       # Number of boosting rounds\n",
    "    max_depth=6,            # Maximum tree depth for base learners\n",
    "    learning_rate=0.05,      # Boosting learning rate\n",
    "    objective='binary:logistic',  # Binary classification\n",
    "    eval_metric='aucpr',    # Evaluation metric AUC-PR\n",
    "    use_label_encoder=False, # To avoid warnings in newer versions of xgboost\n",
    "    n_jobs = 12,\n",
    "    device = \"cuda\",\n",
    "    verbosity = 1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_val_pred = xgb_model.predict(X_val)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"Trainset\\n\", calculate_metric(None, None, 'cls_report', y_train, y_train_pred))\n",
    "print(\"Valset\\n\", calculate_metric(None, None, 'cls_report', y_val, y_val_pred))\n",
    "print(\"Testset\\n\", calculate_metric(None, None, 'cls_report', y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 30\n",
    "hidden_size = 256\n",
    "num_layers = 4\n",
    "dropout_rate = .3\n",
    "\n",
    "lr = 3e-4\n",
    "num_epochs = 20\n",
    "\n",
    "exp_name = \"smote+mlp\"\n",
    "model = MLP(input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout_rate=dropout_rate,)\n",
    "\n",
    "train_loss_hist, train_acc_hist, val_acc_hist, model = train(model, a_smote_train_dataloader, val_dataloader, \n",
    "      num_epochs=num_epochs, optimizer=torch.optim.Adam(model.parameters(), lr=lr), \n",
    "      criterion=torch.nn.BCELoss(), seed=53, metric=\"f1\", )\n",
    "\n",
    "evaluate_model(exp_name, model, train_loss_hist, train_acc_hist, val_acc_hist, test_dataloader, \"cls_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w/ re-sampling (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original trainset distribution Counter({0.0: 226013, 1.0: 372})\n",
      "sampled 225641 # of fraud examples using VAE\n",
      "after applying VAE: Counter({0.0: 226013, 1.0: 226013})\n"
     ]
    }
   ],
   "source": [
    "from imbalance import augment_trainset\n",
    "a_vae_train_dataset = augment_trainset(train_dataset, method=\"vae\")\n",
    "a_vae_train_dataloader = make_dataloader(batch_size, a_vae_train_dataset, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, y_train = a_vae_train_dataset.tensors[0].numpy(), a_vae_train_dataset.tensors[1].numpy()\n",
    "X_val, y_val = val_dataset.tensors[0].numpy(), val_dataset.tensors[1].numpy()\n",
    "X_test, y_test = test_dataset.tensors[0].numpy(), test_dataset.tensors[1].numpy()\n",
    "\n",
    "log_reg = LogisticRegression(penalty='l2', C=1.0, max_iter=100000, random_state=53,)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_val_pred = log_reg.predict(X_val)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "print(\"Trainset\\n\", calculate_metric(None, None, 'cls_report', y_train, y_train_pred))\n",
    "print(\"Valset\\n\", calculate_metric(None, None, 'cls_report', y_val, y_val_pred))\n",
    "print(\"Testset\\n\", calculate_metric(None, None, 'cls_report', y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=3000,       # Number of boosting rounds\n",
    "    max_depth=6,            # Maximum tree depth for base learners\n",
    "    learning_rate=0.05,      # Boosting learning rate\n",
    "    objective='binary:logistic',  # Binary classification\n",
    "    eval_metric='aucpr',    # Evaluation metric AUC-PR\n",
    "    use_label_encoder=False, # To avoid warnings in newer versions of xgboost\n",
    "    n_jobs = 12,\n",
    "    device = \"cuda\",\n",
    "    verbosity = 1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_val_pred = xgb_model.predict(X_val)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"Trainset\\n\", calculate_metric(None, None, 'cls_report', y_train, y_train_pred))\n",
    "print(\"Valset\\n\", calculate_metric(None, None, 'cls_report', y_val, y_val_pred))\n",
    "print(\"Testset\\n\", calculate_metric(None, None, 'cls_report', y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 30\n",
    "hidden_size = 256\n",
    "num_layers = 4\n",
    "dropout_rate = .3\n",
    "\n",
    "lr = 3e-4\n",
    "num_epochs = 20\n",
    "\n",
    "exp_name = \"vae+mlp\"\n",
    "model = MLP(input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout_rate=dropout_rate,)\n",
    "\n",
    "train_loss_hist, train_acc_hist, val_acc_hist, model = train(model, a_vae_train_dataloader, val_dataloader, \n",
    "      num_epochs=num_epochs, optimizer=torch.optim.Adam(model.parameters(), lr=lr), \n",
    "      criterion=torch.nn.BCELoss(), seed=53, metric=\"f1\", )\n",
    "\n",
    "evaluate_model(exp_name, model, train_loss_hist, train_acc_hist, val_acc_hist, test_dataloader, \"cls_report\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae_fraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
